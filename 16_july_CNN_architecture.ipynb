{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424415d-e4ac-4d05-8b48-6c60673e798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb5854-5daa-46f7-84ac-59f04a8daf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pooling, in the context of Convolutional Neural Networks (CNN), is a technique used to downsample the spatial dimensions of feature maps. It serves the purpose of reducing the spatial size of the input representation, extracting dominant features, and facilitating translation invariance. Here are the purpose and benefits of pooling in CNN:\n",
    "\n",
    "1. Dimension Reduction: Pooling reduces the spatial dimensions of the feature maps, which helps to reduce the computational complexity and memory requirements of the subsequent layers in the network. By downsampling the feature maps, pooling enables the network to process larger input sizes efficiently.\n",
    "\n",
    "2. Translation Invariance: Pooling provides translation invariance, meaning that small spatial translations in the input result in the same pooled output. This property allows CNNs to recognize patterns or features regardless of their specific position in the input image. Pooling achieves this by summarizing the local information within a pooling region and considering only the most salient features.\n",
    "\n",
    "3. Feature Extraction: Pooling helps to extract the most relevant and dominant features from the input data. By selecting the most important features within each pooling region, pooling focuses on capturing the essential characteristics of the input while discarding less informative details. This enhances the ability of the network to identify and discriminate between different patterns or objects.\n",
    "\n",
    "4. Spatial Invariance: Pooling contributes to spatial invariance, where the network becomes less sensitive to small variations or distortions in the input. This property is desirable when dealing with inputs that may have slight spatial transformations, such as rotated or translated images. Pooling allows the network to recognize the same features regardless of their precise location or orientation.\n",
    "\n",
    "5. Parameter Efficiency: Pooling reduces the number of parameters in the network. By downsampling the feature maps, pooling reduces the spatial resolution and consequently reduces the number of parameters required in subsequent layers. This leads to a more parameter-efficient network architecture, enabling better generalization and reducing the risk of overfitting.\n",
    "\n",
    "6. Computational Efficiency: Pooling reduces the computational complexity of the network. By reducing the spatial size of the feature maps, pooling reduces the number of computations required for subsequent layers. This speeds up the training and inference process, making CNNs more computationally efficient.\n",
    "\n",
    "Pooling operations, such as max pooling or average pooling, play a crucial role in CNN architectures by reducing spatial dimensions, extracting relevant features, achieving translation invariance, and improving computational and parameter efficiency. These benefits contribute to the overall performance and effectiveness of CNN models in various computer vision tasks, such as image classification, object detection, and semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d0200-34f8-4543-84b3-d61f0cb752b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3d82d-63e7-4dbb-9b30-73a7bdf7a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both min pooling and max pooling are common operations in convolutional neural networks (CNNs) used for dimensionality reduction and feature extraction. They are typically applied after convolutional layers to downsample feature maps.\n",
    "\n",
    "The main difference between min pooling and max pooling lies in how they select the representative value from a pool of values.\n",
    "\n",
    "1. Max Pooling:\n",
    "   - Max pooling takes a pool of values and outputs the maximum value from that pool.\n",
    "   - It is used to capture the most salient features within a region of the input.\n",
    "   - By selecting the maximum value, it emphasizes the presence of a particular feature, which helps in detecting patterns, edges, or textures that are most dominant within the pool.\n",
    "   - Max pooling is often used to reduce the spatial dimensions of feature maps while retaining the most important information.\n",
    "\n",
    "2. Min Pooling:\n",
    "   - Min pooling, on the other hand, takes a pool of values and outputs the minimum value from that pool.\n",
    "   - It is less commonly used compared to max pooling.\n",
    "   - Min pooling can help detect features that exhibit low values or represent a specific absence of a certain pattern.\n",
    "   - It can be useful in certain scenarios, such as anomaly detection or identifying regions with minimal activity.\n",
    "\n",
    "In both cases, pooling is performed by sliding a window (typically non-overlapping) over the input feature map and applying the pooling operation within each window. This process reduces the spatial dimensions while retaining the most relevant information.\n",
    "\n",
    "In summary, max pooling focuses on capturing the most prominent features, while min pooling aims to identify low-value or absence patterns. The choice between the two depends on the specific task and the characteristics of the data being processed. In most CNN architectures, max pooling is more commonly used due to its effectiveness in capturing dominant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0272d-eb4b-45c8-a2fb-f97209f0c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d6446-a233-4734-9cd3-875b98e78b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Padding in convolutional neural networks (CNNs) refers to the process of adding extra border pixels around the input image or feature map. These additional pixels are typically filled with zeros, hence the name \"zero-padding.\" The primary purpose of padding is to control the spatial dimensions of the output feature maps after convolutional operations.\n",
    "\n",
    "Padding is significant for several reasons:\n",
    "\n",
    "1. Preservation of spatial information: Convolutional layers reduce the spatial dimensions of the input due to the application of filters (kernels) that slide over the input. Without padding, the border pixels of the input receive fewer convolutions, leading to a reduction in spatial information at the edges of the feature maps. Padding ensures that all pixels in the input have the same opportunity to contribute to the output feature maps, preserving spatial information and preventing a loss of information near the borders.\n",
    "\n",
    "2. Retaining spatial resolution: In many cases, preserving the spatial resolution is crucial. For example, in object detection tasks, it is important to precisely localize objects within an image. Padding helps maintain the original spatial resolution of the input, allowing the network to detect objects at different locations accurately.\n",
    "\n",
    "3. Mitigating information loss: As the receptive field (the area in the input that a single filter \"sees\") increases deeper into the network, the spatial dimensions of the feature maps decrease. Without padding, the size of the receptive field would continue to shrink, leading to a loss of fine-grained spatial details. Padding counteracts this effect, ensuring that the receptive field remains constant across different layers and reducing the risk of information loss.\n",
    "\n",
    "4. Border effects: When applying convolutional operations near the borders of the input, the filter might not entirely fit within the input's spatial dimensions, resulting in incomplete convolutions. Padding alleviates this issue by extending the input's borders, enabling complete convolutions across the entire input space.\n",
    "\n",
    "The amount of padding applied is determined by the desired output spatial dimensions and the size of the convolutional filters. It can be calculated using various formulas, such as the \"same\" padding, which pads the input in such a way that the output feature maps have the same spatial dimensions as the input, or \"valid\" padding, which does not apply any padding, resulting in smaller output feature maps.\n",
    "\n",
    "In summary, padding in CNNs plays a crucial role in maintaining spatial information, preserving resolution, mitigating information loss, and handling border effects. It ensures that the network can effectively learn and represent features across the entire input space, leading to more accurate and reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26208504-4ef7-44b7-b2a4-7f97cb890293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6bdf5-8442-441d-8788-2af0f36050ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! Let's compare and contrast the effects of using zero-padding and valid-padding on the output feature map size.\n",
    "\n",
    "1. Zero-padding:\n",
    "   - Zero-padding refers to adding extra border pixels around the input image or feature map, typically filled with zeros.\n",
    "   - With zero-padding, the output feature map size can be calculated using the formula: \n",
    "     output_size = (input_size + 2 * padding - filter_size) / stride + 1\n",
    "   - Zero-padding increases the spatial dimensions of the input, which results in larger output feature maps compared to the input size.\n",
    "   - The added border pixels provide a buffer zone that allows the convolutional filters to capture information near the borders of the input.\n",
    "   - Zero-padding helps preserve spatial information, retain spatial resolution, mitigate information loss, and handle border effects.\n",
    "   - Zero-padding is commonly used in convolutional layers to maintain consistent spatial dimensions throughout the network and prevent information loss at the edges.\n",
    "\n",
    "2. Valid-padding:\n",
    "   - Valid-padding, also known as \"no padding,\" means that no extra border pixels are added around the input.\n",
    "   - With valid-padding, the output feature map size can be calculated using the formula:\n",
    "     output_size = (input_size - filter_size) / stride + 1\n",
    "   - Valid-padding reduces the spatial dimensions of the input, which leads to smaller output feature maps compared to the input size.\n",
    "   - Without padding, the convolutional filters do not extend beyond the boundaries of the input, resulting in a smaller receptive field.\n",
    "   - Valid-padding discards information near the borders of the input, potentially causing a loss of spatial details.\n",
    "   - Valid-padding is often used when the preservation of spatial resolution is not a primary concern or when the input size is large enough to mitigate border effects.\n",
    "\n",
    "In summary:\n",
    "- Zero-padding increases the output feature map size, while valid-padding reduces it.\n",
    "- Zero-padding helps preserve spatial information, retain resolution, and mitigate information loss, but it also increases computational complexity.\n",
    "- Valid-padding reduces the spatial dimensions, discards information near the borders, and can lead to a smaller receptive field.\n",
    "- The choice between zero-padding and valid-padding depends on the specific requirements of the task, such as the need for spatial accuracy, sensitivity to border effects, or computational constraints.\n",
    "\n",
    "It's important to note that the choice of padding and its effects on the output feature map size can have implications for subsequent layers and the overall performance of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142c5c3-315e-4f9c-95d9-4661928e1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d9a0f-ba29-4266-884c-dd23fe4f1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues in the 1990s. It was primarily designed for handwritten digit recognition tasks, such as recognizing digits in postal addresses.\n",
    "\n",
    "Here's an overview of the LeNet-5 architecture:\n",
    "\n",
    "1. Input Layer:\n",
    "   - LeNet-5 takes grayscale images of size 32x32 pixels as input.\n",
    "   - The images are typically normalized to have pixel values ranging from 0 to 1.\n",
    "\n",
    "2. Convolutional Layers:\n",
    "   - LeNet-5 consists of two convolutional layers.\n",
    "   - The first convolutional layer applies six filters (kernels) of size 5x5 to the input.\n",
    "   - Each filter performs a convolution operation, resulting in feature maps with reduced spatial dimensions.\n",
    "   - The second convolutional layer applies sixteen filters of size 5x5 to the output of the first layer.\n",
    "   - Again, this produces feature maps with further reduced spatial dimensions.\n",
    "\n",
    "3. Pooling Layers:\n",
    "   - After each convolutional layer, LeNet-5 incorporates a pooling layer.\n",
    "   - The pooling layers use average pooling with a 2x2 filter and a stride of 2.\n",
    "   - Average pooling reduces the spatial dimensions of the feature maps while retaining important features.\n",
    "\n",
    "4. Fully Connected Layers:\n",
    "   - Following the convolutional and pooling layers, LeNet-5 includes three fully connected layers.\n",
    "   - The first fully connected layer has 120 neurons, while the second has 84 neurons.\n",
    "   - These layers help capture high-level features by combining information from the preceding layers.\n",
    "   - The final fully connected layer has ten neurons, corresponding to the ten possible classes (digits 0-9) in digit recognition tasks.\n",
    "   - The activation function used in the fully connected layers is typically a sigmoid or hyperbolic tangent function.\n",
    "\n",
    "5. Output Layer:\n",
    "   - The output layer employs a softmax activation function to produce a probability distribution over the ten possible classes.\n",
    "   - The class with the highest probability is considered the predicted class.\n",
    "\n",
    "LeNet-5's architecture introduced several key concepts that are now widely used in modern CNNs, including the alternating convolutional and pooling layers, the use of non-linear activation functions, and the employment of a softmax output layer for classification tasks.\n",
    "\n",
    "Despite its simplicity by today's standards, LeNet-5 played a crucial role in demonstrating the effectiveness of deep learning in image recognition tasks. It served as a foundation for subsequent advancements in CNN architectures, paving the way for the deep learning revolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacc7ac-cedf-4046-9d60-f09019ea89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1b168-f115-46fb-9357-31cf0fc3c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! The key components of LeNet-5 and their purposes are as follows:\n",
    "\n",
    "1. Convolutional Layers:\n",
    "   - LeNet-5 consists of two convolutional layers.\n",
    "   - The purpose of the convolutional layers is to extract meaningful local features from the input images.\n",
    "   - Each convolutional layer applies a set of learnable filters (kernels) to the input feature maps.\n",
    "   - The convolution operation involves sliding these filters across the input, computing element-wise multiplications, and summing the results to produce feature maps.\n",
    "   - By applying multiple filters, the network learns to detect different patterns and features at various spatial locations.\n",
    "\n",
    "2. Pooling Layers:\n",
    "   - After each convolutional layer, LeNet-5 includes pooling layers.\n",
    "   - The pooling layers serve to reduce the spatial dimensions of the feature maps while preserving the most salient information.\n",
    "   - LeNet-5 employs average pooling, where each pooling unit computes the average value within a defined region (usually 2x2) of the input feature map.\n",
    "   - Average pooling helps to downsample the feature maps, making the network more robust to small variations in the position of features and reducing computational complexity.\n",
    "\n",
    "3. Fully Connected Layers:\n",
    "   - Following the convolutional and pooling layers, LeNet-5 incorporates three fully connected layers.\n",
    "   - The fully connected layers are responsible for capturing high-level abstractions and combining information from the preceding layers.\n",
    "   - The first fully connected layer has 120 neurons, while the second fully connected layer has 84 neurons.\n",
    "   - These layers employ activation functions (commonly sigmoid or hyperbolic tangent) to introduce non-linearity into the network.\n",
    "   - The final fully connected layer consists of ten neurons, representing the ten possible classes in digit recognition.\n",
    "   - The output of this layer is fed into a softmax function to obtain class probabilities.\n",
    "\n",
    "4. Activation Functions:\n",
    "   - Activation functions introduce non-linearity and help the network learn complex relationships between inputs and outputs.\n",
    "   - LeNet-5 typically uses sigmoid or hyperbolic tangent activation functions in its fully connected layers.\n",
    "   - The choice of activation functions enables the network to model non-linear mappings and make predictions based on the learned features.\n",
    "\n",
    "5. Output Layer:\n",
    "   - The output layer of LeNet-5 uses the softmax activation function.\n",
    "   - Softmax converts the outputs of the last fully connected layer into a probability distribution over the possible classes.\n",
    "   - The class with the highest probability is considered the predicted class.\n",
    "\n",
    "In summary, the convolutional layers extract local features, pooling layers reduce spatial dimensions, fully connected layers capture high-level abstractions, activation functions introduce non-linearity, and the output layer provides class probabilities. Together, these components enable LeNet-5 to perform digit recognition tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583c761-325e-48f0-ac97-a27ef947c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7e0c7-fa48-47e3-8957-0e9238eb89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet-5, being one of the early pioneering CNN architectures, has both advantages and limitations in the context of image classification tasks. Let's explore them:\n",
    "\n",
    "Advantages of LeNet-5:\n",
    "\n",
    "1. Efficiency with small-sized inputs: LeNet-5 was designed to process small-sized grayscale images, such as 32x32 pixels. Its architecture is optimized for such inputs, making it computationally efficient and requiring fewer parameters compared to architectures designed for larger images.\n",
    "\n",
    "2. Hierarchical feature learning: LeNet-5's architecture, with alternating convolutional and pooling layers, allows for hierarchical feature learning. The early convolutional layers capture low-level local features, while the subsequent layers learn increasingly complex and abstract representations. This enables the network to understand hierarchical structures in images.\n",
    "\n",
    "3. Translation invariance: The use of pooling layers in LeNet-5 helps achieve translation invariance. By downsampling the feature maps, the network becomes less sensitive to small translations of the input image, making it robust to slight variations in the position of features.\n",
    "\n",
    "4. Simplicity and interpretability: LeNet-5 has a relatively simple architecture compared to modern CNNs. Its simplicity makes it easier to understand and interpret. This characteristic is particularly valuable in educational settings or scenarios where model interpretability is crucial.\n",
    "\n",
    "Limitations of LeNet-5:\n",
    "\n",
    "1. Limited capacity: LeNet-5's architecture may not have sufficient capacity to handle more complex and larger-scale datasets. It was primarily designed for digit recognition tasks, which have relatively simpler patterns. The shallow architecture and small receptive fields limit its ability to learn intricate features and handle more diverse image datasets.\n",
    "\n",
    "2. Lack of scalability: Due to its specific design for small-sized inputs, LeNet-5 may not scale well to larger images. The receptive fields and pooling operations may not adequately capture contextual information in bigger images, affecting its performance on tasks that require a broader field of view.\n",
    "\n",
    "3. Lack of advanced techniques: LeNet-5 predates many modern advancements in CNN architectures. It does not incorporate more recent techniques like batch normalization, residual connections, or advanced activation functions, which have been shown to improve performance in image classification tasks.\n",
    "\n",
    "4. Not suitable for complex datasets: LeNet-5's architecture and capacity make it less suitable for complex image datasets with a high level of variation and intricacies. It may struggle to capture fine-grained details or handle complex object recognition tasks that require more sophisticated models.\n",
    "\n",
    "In summary, while LeNet-5 was groundbreaking in its time and laid the foundation for CNNs, it has some limitations when applied to more complex image classification tasks. It excels with small-sized inputs, provides interpretability, and achieves translation invariance but may lack the capacity and scalability needed for handling larger and more complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c2989-5246-4456-ab46-e2ef6ebcd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a9fc45-83f5-48ab-bd29-6d4392e2c65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.8)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.11)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.3 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee7b161-d72f-4d5f-ad00-da49020cc7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 16:09:32.174838: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-16 16:09:32.238503: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-16 16:09:32.240530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 16:09:33.346621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 6s 10ms/step - loss: 0.3202 - accuracy: 0.9047 - val_loss: 0.0950 - val_accuracy: 0.9714\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0894 - accuracy: 0.9724 - val_loss: 0.0549 - val_accuracy: 0.9826\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0623 - accuracy: 0.9806 - val_loss: 0.0458 - val_accuracy: 0.9845\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0508 - accuracy: 0.9843 - val_loss: 0.0503 - val_accuracy: 0.9839\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0428 - accuracy: 0.9866 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0363 - accuracy: 0.9883 - val_loss: 0.0403 - val_accuracy: 0.9868\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0305 - accuracy: 0.9905 - val_loss: 0.0350 - val_accuracy: 0.9888\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.0292 - val_accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.0329 - val_accuracy: 0.9892\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 0.0377 - val_accuracy: 0.9884\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9884\n",
      "Test Loss: 0.037744369357824326\n",
      "Test Accuracy: 0.9883999824523926\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "num_classes = 10\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2baafbf-22dd-41d1-9341-eb64fa0537b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fb3a4-0de5-48f5-ac4d-74d53de88aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet is a popular convolutional neural network (CNN) architecture that achieved significant breakthroughs in image classification, specifically winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. Input Layer:\n",
    "   - AlexNet takes RGB images of size 227x227 pixels as input.\n",
    "   - The images are typically normalized to have pixel values ranging from 0 to 1.\n",
    "\n",
    "2. Convolutional Layers:\n",
    "   - AlexNet starts with five convolutional layers.\n",
    "   - The first convolutional layer applies 96 filters of size 11x11 with a stride of 4.\n",
    "   - The subsequent convolutional layers apply different numbers of filters: 256, 384, 384, and 256.\n",
    "   - The activation function used is a rectified linear unit (ReLU), introducing non-linearity.\n",
    "\n",
    "3. Max Pooling Layers:\n",
    "   - After each of the first two convolutional layers, AlexNet includes max pooling layers.\n",
    "   - The max pooling layers use a 3x3 filter with a stride of 2.\n",
    "   - Max pooling reduces the spatial dimensions of the feature maps while retaining the most prominent features.\n",
    "\n",
    "4. Local Response Normalization (LRN) Layer:\n",
    "   - Between the convolutional and pooling layers, AlexNet incorporates LRN layers.\n",
    "   - LRN layers aim to provide local competition between adjacent neurons and enhance generalization.\n",
    "   - LRN normalizes the responses of the neurons across channels, promoting the detection of more diverse and robust features.\n",
    "\n",
    "5. Fully Connected Layers:\n",
    "   - Following the convolutional and pooling layers, AlexNet includes three fully connected layers.\n",
    "   - The first fully connected layer has 4096 neurons, while the subsequent two layers have 4096 and 1000 neurons, respectively.\n",
    "   - Dropout regularization is applied after each fully connected layer to prevent overfitting.\n",
    "   - The activation function used in the fully connected layers is ReLU.\n",
    "\n",
    "6. Output Layer:\n",
    "   - The output layer of AlexNet consists of 1000 neurons, representing the 1000 possible classes in the ILSVRC challenge.\n",
    "   - The activation function used in the output layer is softmax, which produces a probability distribution over the classes.\n",
    "\n",
    "7. Overlapping Prediction and Training:\n",
    "   - One unique aspect of AlexNet is that it splits the training of the network over two GPUs and merges the results during prediction.\n",
    "   - This approach helps to mitigate memory limitations and allows for efficient training and inference on large-scale datasets.\n",
    "\n",
    "In summary, AlexNet revolutionized the field of image classification with its deep architecture, ReLU activations, dropout regularization, and large-scale training on the ImageNet dataset. Its success paved the way for the development of deeper and more powerful CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b189309-fa5b-476f-bbb3-16ca6640eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7d26c-ea0f-4294-ab1c-10f2e3113379",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet introduced several architectural innovations that contributed to its breakthrough performance. These innovations include:\n",
    "\n",
    "1. Deep Architecture:\n",
    "   - AlexNet was one of the first CNN architectures to have a deep structure with multiple layers.\n",
    "   - Prior to AlexNet, shallow networks were commonly used, but AlexNet demonstrated the power of deeper architectures in learning complex features and improving classification accuracy.\n",
    "\n",
    "2. Convolutional Layers with Large Filter Sizes:\n",
    "   - AlexNet utilized convolutional layers with large filter sizes, particularly the first layer with an 11x11 filter.\n",
    "   - This choice allowed the network to capture more spatial context and capture larger-scale patterns in the input images.\n",
    "\n",
    "3. Rectified Linear Units (ReLU):\n",
    "   - AlexNet adopted the rectified linear unit (ReLU) activation function instead of traditional activation functions like sigmoid or tanh.\n",
    "   - ReLU provides faster and more efficient training by mitigating the vanishing gradient problem and enabling better gradient flow during backpropagation.\n",
    "\n",
    "4. Local Response Normalization (LRN):\n",
    "   - AlexNet incorporated LRN layers between the convolutional and pooling layers.\n",
    "   - LRN layers encouraged competition among adjacent neurons, promoting the detection of diverse and robust features.\n",
    "   - This normalization technique enhanced the generalization ability of the network and contributed to improved accuracy.\n",
    "\n",
    "5. Overlapping Pooling:\n",
    "   - AlexNet introduced the concept of overlapping pooling by using a stride smaller than the pooling window size.\n",
    "   - Overlapping pooling reduced the information loss during downscaling, preserving more spatial details and improving the ability of the network to capture fine-grained features.\n",
    "\n",
    "6. Dropout Regularization:\n",
    "   - AlexNet employed dropout regularization after each fully connected layer.\n",
    "   - Dropout randomly drops out a fraction of the neurons during training, reducing overfitting and improving the network's generalization ability.\n",
    "\n",
    "7. GPU Acceleration and Parallelization:\n",
    "   - AlexNet was designed to take advantage of GPU acceleration and parallel processing.\n",
    "   - The network split the training process across two GPUs and merged the results during prediction, allowing for efficient training and inference on large-scale datasets.\n",
    "\n",
    "These architectural innovations collectively contributed to the breakthrough performance of AlexNet. They enabled the network to learn rich and discriminative features, effectively handle large-scale datasets, and achieve a significant improvement in accuracy on the challenging ImageNet dataset. AlexNet's success inspired further research in deep learning and paved the way for subsequent advancements in CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a69f4-7fba-4a6b-a7d0-a4c02cf0d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19423ce-a630-49ba-9a39-aa3af1036aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "In AlexNet, convolutional layers, pooling layers, and fully connected layers each play a crucial role in the architecture:\n",
    "\n",
    "1. Convolutional Layers:\n",
    "   - Convolutional layers in AlexNet are responsible for learning local patterns and extracting features from the input images.\n",
    "   - The use of multiple convolutional layers allows the network to capture increasingly complex and abstract features.\n",
    "   - AlexNet introduced large filter sizes, such as the 11x11 filters in the first layer, which helps capture larger-scale patterns and spatial context.\n",
    "   - These layers employ the rectified linear unit (ReLU) activation function, promoting non-linearity and better gradient flow during training.\n",
    "\n",
    "2. Pooling Layers:\n",
    "   - Pooling layers in AlexNet follow the convolutional layers and serve to downsample the feature maps, reducing their spatial dimensions.\n",
    "   - The pooling operation helps extract the most salient features while reducing the network's sensitivity to small spatial variations.\n",
    "   - AlexNet incorporates max pooling layers with a 3x3 filter and a stride of 2, which downsamples the feature maps while retaining important information.\n",
    "   - Overlapping pooling, achieved by using a smaller stride than the pooling window size, preserves more spatial details and improves the network's ability to capture fine-grained features.\n",
    "\n",
    "3. Fully Connected Layers:\n",
    "   - Fully connected layers in AlexNet are responsible for capturing high-level abstractions and making class predictions.\n",
    "   - They take the learned features from the preceding layers and combine them to make predictions.\n",
    "   - AlexNet includes three fully connected layers with a decreasing number of neurons (4096, 4096, and 1000).\n",
    "   - ReLU activation functions are used in these layers, introducing non-linearity and allowing the network to learn complex mappings.\n",
    "   - Dropout regularization is applied after each fully connected layer to prevent overfitting by randomly dropping out a fraction of the neurons during training.\n",
    "\n",
    "The convolutional layers extract local features, capturing patterns at different spatial scales. The pooling layers reduce the spatial dimensions and downsample the feature maps, retaining the most salient information. Finally, the fully connected layers capture high-level abstractions and make predictions based on the learned features. The combination of these layers allows AlexNet to effectively learn discriminative features and achieve high accuracy in image classification tasks.\n",
    "\n",
    "It's important to note that these components work together in a hierarchical manner, with each layer building upon the representations learned by the previous layers. This hierarchical feature learning is a key factor in the success of CNN architectures like AlexNet in handling complex visual tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065bb45-102e-4676-9c40-90422dd4845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec3a58-ad9e-49cf-948e-6775cfee2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define AlexNet architecture\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = AlexNet().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf91975-34a9-4833-bc15-083299a013b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178f1ca-42b5-4a2b-ae80-fc4cb11bbc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a3925-673e-4ca2-be79-6d07bf65d0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcaf05d-15be-42f0-a623-9b60a233784c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26230611-475c-43f7-8e82-ab61687501db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8de41-8d8d-41fb-aa47-36ee38d598b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8d048-c98c-4d71-aed9-930335ebf008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba59ffe-ae21-45d6-a846-b2ff4c3d50eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cd870-e0ba-4e31-a740-f512066a53e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e862e0b-bfa4-47d0-8e5f-112b80d502b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7512532-7bba-4852-91c1-30da8773ac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200945a5-e859-4336-ad71-facfd3bef9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9dc3e7-9653-4230-b816-cea9af24d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306eb1bd-a271-44c6-9453-75a5afaed27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331ed0f-8e53-485b-b824-8f0bc54b8386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66739285-ffba-4f40-8143-9e20423c8ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717ffed-008c-47d9-abf3-ed91d46c9cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a59c8c-b546-4feb-ad40-871b5efba7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7df68c-d482-4e0a-99db-8bfa922c0ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
